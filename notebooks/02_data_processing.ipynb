{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š Football Player Analytics Pipeline\n",
                "## Notebook 2: Data Processing (ALL FEATURES)\n",
                "\n",
                "This notebook:\n",
                "1. Loads ALL stat types (standard, shooting, passing, possession, gca, misc)\n",
                "2. Merges them into one comprehensive dataset\n",
                "3. Filters for forwards/attackers\n",
                "4. Uses ALL available per-90 features for clustering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "DATA_DIR = Path(\"../data\")\n",
                "RAW_DIR = DATA_DIR / \"raw\"\n",
                "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
                "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"âœ… Libraries loaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check Available Data Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all CSV files in raw directory\n",
                "csv_files = list(RAW_DIR.glob(\"*.csv\"))\n",
                "print(f\"ðŸ“ Found {len(csv_files)} CSV files:\")\n",
                "for f in sorted(csv_files):\n",
                "    size_kb = f.stat().st_size / 1024\n",
                "    print(f\"  â€¢ {f.name} ({size_kb:.1f} KB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Merge All Stat Types"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_column_name(col):\n",
                "    \"\"\"Clean multi-level FBref column names\"\"\"\n",
                "    col = str(col)\n",
                "    # Remove 'Unnamed' prefix\n",
                "    if 'Unnamed' in col:\n",
                "        parts = col.split('_')\n",
                "        return parts[-1] if len(parts) > 1 else col\n",
                "    return col\n",
                "\n",
                "def load_and_clean_csv(filepath):\n",
                "    \"\"\"Load CSV and clean column names\"\"\"\n",
                "    df = pd.read_csv(filepath)\n",
                "    df.columns = [clean_column_name(c) for c in df.columns]\n",
                "    return df\n",
                "\n",
                "# Check for all_leagues_master.csv (standard stats)\n",
                "master_file = RAW_DIR / \"all_leagues_master.csv\"\n",
                "\n",
                "if master_file.exists():\n",
                "    df = load_and_clean_csv(master_file)\n",
                "    print(f\"âœ… Loaded standard stats: {len(df)} players, {len(df.columns)} columns\")\n",
                "else:\n",
                "    raise FileNotFoundError(\"Run Notebook 01 first to scrape data!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try to load additional stat types and merge\n",
                "stat_files = {\n",
                "    'shooting': RAW_DIR / \"all_leagues_shooting.csv\",\n",
                "    'passing': RAW_DIR / \"all_leagues_passing.csv\",\n",
                "    'possession': RAW_DIR / \"all_leagues_possession.csv\",\n",
                "    'gca': RAW_DIR / \"all_leagues_gca.csv\",\n",
                "    'misc': RAW_DIR / \"all_leagues_misc.csv\",\n",
                "    'defense': RAW_DIR / \"all_leagues_defense.csv\"\n",
                "}\n",
                "\n",
                "# Find player column in main df\n",
                "player_col = [c for c in df.columns if 'player' in c.lower()][0]\n",
                "print(f\"\\nPlayer column: {player_col}\")\n",
                "\n",
                "for stat_name, filepath in stat_files.items():\n",
                "    if filepath.exists():\n",
                "        stat_df = load_and_clean_csv(filepath)\n",
                "        stat_player_col = [c for c in stat_df.columns if 'player' in c.lower()][0]\n",
                "        \n",
                "        # Get new columns (not in main df)\n",
                "        existing_cols = set(df.columns)\n",
                "        new_cols = [c for c in stat_df.columns if c not in existing_cols]\n",
                "        \n",
                "        if new_cols:\n",
                "            merge_cols = [stat_player_col] + new_cols\n",
                "            stat_df_subset = stat_df[merge_cols].copy()\n",
                "            stat_df_subset = stat_df_subset.rename(columns={stat_player_col: player_col})\n",
                "            \n",
                "            # Merge on player name\n",
                "            df = df.merge(stat_df_subset, on=player_col, how='left')\n",
                "            print(f\"âœ… Merged {stat_name}: +{len(new_cols)} new columns\")\n",
                "    else:\n",
                "        print(f\"âš ï¸ {stat_name}: file not found (run 01b to scrape)\")\n",
                "\n",
                "print(f\"\\nðŸ“Š Total columns after merge: {len(df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show all columns\n",
                "print(\"ðŸ“‹ All available columns:\")\n",
                "for i, col in enumerate(df.columns):\n",
                "    print(f\"  {i+1:2d}. {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Filter for Forwards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find position column\n",
                "pos_col = [c for c in df.columns if 'pos' in c.lower()][0]\n",
                "print(f\"Position column: {pos_col}\")\n",
                "print(f\"\\nPosition distribution:\")\n",
                "print(df[pos_col].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter for forwards (any position containing FW)\n",
                "mask = df[pos_col].astype(str).str.upper().str.contains('FW', na=False)\n",
                "forwards_df = df[mask].copy()\n",
                "\n",
                "print(f\"âœ… Filtered: {len(df)} â†’ {len(forwards_df)} forwards\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Filter for Minimum Minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find minutes column\n",
                "min_col = None\n",
                "for col in forwards_df.columns:\n",
                "    if 'min' in col.lower() and '90' not in col.lower():\n",
                "        min_col = col\n",
                "        break\n",
                "\n",
                "if min_col:\n",
                "    forwards_df[min_col] = pd.to_numeric(\n",
                "        forwards_df[min_col].astype(str).str.replace(',', ''), \n",
                "        errors='coerce'\n",
                "    )\n",
                "    \n",
                "    MIN_MINUTES = 450  # ~5 full games\n",
                "    forwards_df = forwards_df[forwards_df[min_col] >= MIN_MINUTES]\n",
                "    print(f\"âœ… After minimum {MIN_MINUTES} minutes: {len(forwards_df)} forwards\")\n",
                "else:\n",
                "    print(\"âš ï¸ Could not find minutes column\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Identify ALL Per-90 Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all per-90 columns (already normalized by playing time)\n",
                "per90_cols = [c for c in forwards_df.columns if '90' in c.lower() or 'per90' in c.lower()]\n",
                "\n",
                "print(f\"ðŸ“Š Found {len(per90_cols)} per-90 columns:\")\n",
                "for col in per90_cols:\n",
                "    print(f\"  â€¢ {col}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Also find raw counting stats that we can convert to per-90\n",
                "# Get 90s column for normalization\n",
                "ninety_col = [c for c in forwards_df.columns if c.lower() == '90s' or '90s' in c.lower()][0]\n",
                "forwards_df[ninety_col] = pd.to_numeric(forwards_df[ninety_col], errors='coerce')\n",
                "\n",
                "print(f\"\\nUsing '{ninety_col}' for per-90 conversion\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define which raw columns to convert to per-90\n",
                "# These are counting stats that should be normalized\n",
                "COUNTING_STATS = [\n",
                "    'Gls', 'Ast', 'G+A', 'G-PK', 'PK', 'CrdY', 'CrdR',  # Performance\n",
                "    'xG', 'npxG', 'xAG', 'npxG+xAG',  # Expected\n",
                "    'PrgC', 'PrgP', 'PrgR',  # Progression\n",
                "    'Sh', 'SoT', 'Dist',  # Shooting\n",
                "    'KP', 'TB', 'Sw', 'Crs', 'CK',  # Passing\n",
                "    'Att', 'Succ', 'Tkld',  # Dribbles/Carries\n",
                "    'SCA', 'GCA',  # Goal/Shot creating actions\n",
                "    'Won', 'Lost',  # Aerials\n",
                "    'Fls', 'Fld',  # Fouls\n",
                "]\n",
                "\n",
                "# Convert each to per-90\n",
                "converted_cols = []\n",
                "for stat in COUNTING_STATS:\n",
                "    # Find matching column (case-insensitive)\n",
                "    matching = [c for c in forwards_df.columns if c.lower() == stat.lower() or c.endswith('_' + stat)]\n",
                "    \n",
                "    for col in matching:\n",
                "        new_col = f\"{col}_per90\"\n",
                "        if new_col not in forwards_df.columns and col not in per90_cols:\n",
                "            forwards_df[col] = pd.to_numeric(forwards_df[col], errors='coerce')\n",
                "            forwards_df[new_col] = forwards_df[col] / forwards_df[ninety_col].replace(0, np.nan)\n",
                "            converted_cols.append(new_col)\n",
                "\n",
                "print(f\"\\nâœ… Converted {len(converted_cols)} stats to per-90:\")\n",
                "for col in converted_cols:\n",
                "    print(f\"  + {col}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FINAL: Get ALL features for clustering\n",
                "# Include existing per-90 columns + newly converted ones\n",
                "\n",
                "all_per90_cols = per90_cols + converted_cols\n",
                "\n",
                "# Remove duplicates and non-numeric columns\n",
                "CLUSTERING_FEATURES = []\n",
                "for col in all_per90_cols:\n",
                "    if col in forwards_df.columns:\n",
                "        # Check if numeric\n",
                "        if pd.api.types.is_numeric_dtype(forwards_df[col]):\n",
                "            if col not in CLUSTERING_FEATURES:\n",
                "                CLUSTERING_FEATURES.append(col)\n",
                "\n",
                "print(f\"\\nðŸ“Š FINAL CLUSTERING FEATURES ({len(CLUSTERING_FEATURES)}):\")\n",
                "for i, col in enumerate(CLUSTERING_FEATURES, 1):\n",
                "    print(f\"  {i:2d}. {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Handle Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check missing values\n",
                "print(\"ðŸ“Š Missing values in clustering features:\")\n",
                "for col in CLUSTERING_FEATURES:\n",
                "    missing = forwards_df[col].isna().sum()\n",
                "    pct = missing / len(forwards_df) * 100\n",
                "    if pct > 0:\n",
                "        print(f\"  {col}: {missing} ({pct:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fill missing values with 0 (conservative - means they didn't do that action)\n",
                "for col in CLUSTERING_FEATURES:\n",
                "    forwards_df[col] = forwards_df[col].fillna(0)\n",
                "\n",
                "print(\"âœ… Filled missing values with 0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Normalize Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply MinMax scaling (0-1 range)\n",
                "scaler = MinMaxScaler()\n",
                "\n",
                "for col in CLUSTERING_FEATURES:\n",
                "    norm_col = f\"{col}_norm\"\n",
                "    forwards_df[norm_col] = scaler.fit_transform(forwards_df[[col]])\n",
                "\n",
                "print(f\"âœ… Normalized {len(CLUSTERING_FEATURES)} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Preview Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show sample\n",
                "id_cols = [player_col, 'Squad', pos_col, '_league'] if '_league' in forwards_df.columns else [player_col, 'Squad', pos_col]\n",
                "id_cols = [c for c in id_cols if c in forwards_df.columns]\n",
                "\n",
                "display_cols = id_cols + CLUSTERING_FEATURES[:8]  # First 8 features\n",
                "print(f\"ðŸ“Š Sample data ({len(forwards_df)} forwards):\")\n",
                "display(forwards_df[display_cols].head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature statistics\n",
                "print(\"ðŸ“Š Feature statistics:\")\n",
                "display(forwards_df[CLUSTERING_FEATURES].describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed data\n",
                "output_file = PROCESSED_DIR / \"forwards_processed.csv\"\n",
                "forwards_df.to_csv(output_file, index=False)\n",
                "print(f\"ðŸ’¾ Saved: {output_file}\")\n",
                "print(f\"   Rows: {len(forwards_df)}\")\n",
                "print(f\"   Columns: {len(forwards_df.columns)}\")\n",
                "\n",
                "# Save feature list\n",
                "feature_file = PROCESSED_DIR / \"clustering_features.txt\"\n",
                "with open(feature_file, 'w') as f:\n",
                "    f.write('\\n'.join(CLUSTERING_FEATURES))\n",
                "print(f\"ðŸ’¾ Saved feature list: {feature_file}\")\n",
                "print(f\"   {len(CLUSTERING_FEATURES)} features for clustering\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## âœ… Next: Run Notebook 03 for clustering\n",
                "\n",
                "All available per-90 features will be used for clustering."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}