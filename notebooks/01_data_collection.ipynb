{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Football Player Analytics Pipeline\n",
                "## Notebook 1: Data Collection from FBref\n",
                "\n",
                "This notebook collects player statistics from 8 major football leagues.\n",
                "\n",
                "### ‚ö†Ô∏è FBref Bot Protection\n",
                "FBref blocks automated requests (403 Forbidden). We solve this by:\n",
                "1. Using Selenium WebDriver to control a real browser\n",
                "2. Adding realistic delays between requests\n",
                "3. Using proper browser headers\n",
                "\n",
                "### Leagues We'll Scrape:\n",
                "1. Premier League (England)\n",
                "2. La Liga (Spain)\n",
                "3. Serie A (Italy)\n",
                "4. Bundesliga (Germany)\n",
                "5. Ligue 1 (France)\n",
                "6. Championship (England - 2nd tier)\n",
                "7. MLS (USA)\n",
                "8. Eredivisie (Netherlands)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Selenium if not already installed\n",
                "!pip install selenium webdriver-manager"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import time\n",
                "import random\n",
                "from pathlib import Path\n",
                "from io import StringIO\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Selenium imports\n",
                "from selenium import webdriver\n",
                "from selenium.webdriver.chrome.service import Service\n",
                "from selenium.webdriver.chrome.options import Options\n",
                "from selenium.webdriver.common.by import By\n",
                "from selenium.webdriver.support.ui import WebDriverWait\n",
                "from selenium.webdriver.support import expected_conditions as EC\n",
                "from webdriver_manager.chrome import ChromeDriverManager\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === CONFIGURATION ===\n",
                "\n",
                "# FBref URLs for current season stats (no season in URL = current season)\n",
                "# These URLs get the CURRENT season automatically\n",
                "LEAGUE_URLS = {\n",
                "    \"Premier-League\": \"https://fbref.com/en/comps/9/stats/Premier-League-Stats\",\n",
                "    \"La-Liga\": \"https://fbref.com/en/comps/12/stats/La-Liga-Stats\",\n",
                "    \"Serie-A\": \"https://fbref.com/en/comps/11/stats/Serie-A-Stats\", \n",
                "    \"Bundesliga\": \"https://fbref.com/en/comps/20/stats/Bundesliga-Stats\",\n",
                "    \"Ligue-1\": \"https://fbref.com/en/comps/13/stats/Ligue-1-Stats\",\n",
                "    \"Championship\": \"https://fbref.com/en/comps/10/stats/Championship-Stats\",\n",
                "    \"MLS\": \"https://fbref.com/en/comps/22/stats/Major-League-Soccer-Stats\",\n",
                "    \"Eredivisie\": \"https://fbref.com/en/comps/23/stats/Eredivisie-Stats\"\n",
                "}\n",
                "\n",
                "# Ghana Black Stars Forwards\n",
                "GHANA_FORWARDS = [\n",
                "    \"Mohammed Kudus\",\n",
                "    \"Antoine Semenyo\",\n",
                "    \"Jordan Ayew\",\n",
                "    \"Ernest Nuamah\",\n",
                "    \"Osman Bukari\",\n",
                "    \"Abdul Fatawu Issahaku\",\n",
                "    \"Kamaldeen Sulemana\",\n",
                "    \"Ibrahim Osman\",\n",
                "    \"Brandon Thomas-Asante\",\n",
                "    \"I√±aki Williams\",\n",
                "    \"Joseph Paintsil\",\n",
                "    \"Jerry Afriyie\",\n",
                "    \"Christopher Bonsu Baah\"\n",
                "]\n",
                "\n",
                "# Data directories\n",
                "DATA_DIR = Path(\"../data\")\n",
                "RAW_DIR = DATA_DIR / \"raw\"\n",
                "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
                "\n",
                "# Create directories\n",
                "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
                "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"‚úÖ Configuration set!\")\n",
                "print(f\"üìÅ Data directory: {DATA_DIR.absolute()}\")\n",
                "print(f\"üèÜ Leagues to scrape: {len(LEAGUE_URLS)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Selenium Browser\n",
                "\n",
                "We use a real Chrome browser in headless mode to bypass FBref's bot protection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_browser():\n",
                "    \"\"\"\n",
                "    Create a Chrome browser instance with anti-detection settings.\n",
                "    \"\"\"\n",
                "    chrome_options = Options()\n",
                "    \n",
                "    # Run in headless mode (no visible browser window)\n",
                "    chrome_options.add_argument(\"--headless=new\")\n",
                "    \n",
                "    # Anti-detection options\n",
                "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
                "    chrome_options.add_argument(\"--no-sandbox\")\n",
                "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
                "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
                "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
                "    \n",
                "    # Disable automation flags\n",
                "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
                "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
                "    \n",
                "    # Create driver\n",
                "    service = Service(ChromeDriverManager().install())\n",
                "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
                "    \n",
                "    # Remove webdriver property\n",
                "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
                "    \n",
                "    return driver\n",
                "\n",
                "print(\"‚úÖ Browser setup function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_fbref_page(driver, url: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Scrape a FBref stats page using Selenium.\n",
                "    \n",
                "    Args:\n",
                "        driver: Selenium WebDriver instance\n",
                "        url: FBref URL to scrape\n",
                "    \n",
                "    Returns:\n",
                "        DataFrame with player stats\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Navigate to page\n",
                "        driver.get(url)\n",
                "        \n",
                "        # Wait for page to load (wait for stats table)\n",
                "        wait = WebDriverWait(driver, 15)\n",
                "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
                "        \n",
                "        # Add random delay to appear more human\n",
                "        time.sleep(random.uniform(2, 4))\n",
                "        \n",
                "        # Get page HTML\n",
                "        html = driver.page_source\n",
                "        \n",
                "        # Parse tables with pandas\n",
                "        tables = pd.read_html(StringIO(html))\n",
                "        \n",
                "        # Find the main stats table (largest with 'Player' column)\n",
                "        for table in tables:\n",
                "            # Flatten multi-level columns if present\n",
                "            if isinstance(table.columns, pd.MultiIndex):\n",
                "                table.columns = ['_'.join(str(c) for c in col).strip() for col in table.columns]\n",
                "            \n",
                "            # Check for player column\n",
                "            col_str = ' '.join(table.columns.astype(str)).lower()\n",
                "            if 'player' in col_str and len(table) > 10:\n",
                "                # Clean up header rows\n",
                "                player_col = [c for c in table.columns if 'player' in c.lower()][0]\n",
                "                table = table[table[player_col] != 'Player']\n",
                "                table = table.dropna(subset=[player_col])\n",
                "                return table\n",
                "        \n",
                "        print(\"  ‚ö†Ô∏è No suitable table found\")\n",
                "        return pd.DataFrame()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Error: {e}\")\n",
                "        return pd.DataFrame()\n",
                "\n",
                "print(\"‚úÖ Scraper function defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test Scraping (Single Page)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with Premier League\n",
                "print(\"üß™ Testing scraper with Premier League...\")\n",
                "print(\"   (This will open Chrome in background)\\n\")\n",
                "\n",
                "# Create browser\n",
                "driver = create_browser()\n",
                "print(\"‚úÖ Browser created\")\n",
                "\n",
                "# Test scrape\n",
                "test_url = LEAGUE_URLS[\"Premier-League\"]\n",
                "print(f\"üîó URL: {test_url}\")\n",
                "\n",
                "test_df = scrape_fbref_page(driver, test_url)\n",
                "\n",
                "if not test_df.empty:\n",
                "    print(f\"\\n‚úÖ SUCCESS! Got {len(test_df)} players\")\n",
                "    print(f\"\\nüìã Columns ({len(test_df.columns)}):\")\n",
                "    print(test_df.columns.tolist()[:15])\n",
                "    print(\"\\nüîç Sample data:\")\n",
                "    display(test_df.head())\n",
                "else:\n",
                "    print(\"‚ùå Test failed\")\n",
                "\n",
                "# Keep browser open for now"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Full Data Collection\n",
                "\n",
                "Now scrape all 8 leagues. We'll use the same browser session to be more efficient."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === MAIN SCRAPING LOOP ===\n",
                "\n",
                "all_data = []\n",
                "\n",
                "print(\"üöÄ Starting full data collection...\")\n",
                "print(f\"üìä Leagues: {len(LEAGUE_URLS)}\")\n",
                "print(f\"‚è±Ô∏è Estimated time: {len(LEAGUE_URLS) * 10 / 60:.1f} minutes\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "\n",
                "# Create browser if not exists\n",
                "try:\n",
                "    driver.title\n",
                "except:\n",
                "    print(\"Creating new browser...\")\n",
                "    driver = create_browser()\n",
                "\n",
                "for league_name, url in LEAGUE_URLS.items():\n",
                "    print(f\"\\nüèÜ Scraping: {league_name}\")\n",
                "    print(f\"   URL: {url}\")\n",
                "    \n",
                "    df = scrape_fbref_page(driver, url)\n",
                "    \n",
                "    if not df.empty:\n",
                "        print(f\"   ‚úÖ Got {len(df)} players\")\n",
                "        \n",
                "        # Add league column\n",
                "        df['_league'] = league_name\n",
                "        df['_season'] = '2024-2025'  # Current season\n",
                "        \n",
                "        # Save individual file\n",
                "        filename = RAW_DIR / f\"{league_name}_standard.csv\"\n",
                "        df.to_csv(filename, index=False)\n",
                "        print(f\"   üíæ Saved: {filename.name}\")\n",
                "        \n",
                "        all_data.append(df)\n",
                "    else:\n",
                "        print(f\"   ‚ùå Failed to scrape {league_name}\")\n",
                "    \n",
                "    # Delay between leagues (important!)\n",
                "    delay = random.uniform(8, 15)\n",
                "    print(f\"   ‚è≥ Waiting {delay:.1f}s...\")\n",
                "    time.sleep(delay)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ SCRAPING COMPLETE!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Close browser when done\n",
                "try:\n",
                "    driver.quit()\n",
                "    print(\"‚úÖ Browser closed\")\n",
                "except:\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine all data into master file\n",
                "if all_data:\n",
                "    master_df = pd.concat(all_data, ignore_index=True)\n",
                "    master_file = RAW_DIR / \"all_leagues_master.csv\"\n",
                "    master_df.to_csv(master_file, index=False)\n",
                "    \n",
                "    print(f\"\\nüìä MASTER FILE CREATED\")\n",
                "    print(f\"üìÅ Location: {master_file}\")\n",
                "    print(f\"üë• Total player records: {len(master_df)}\")\n",
                "    print(f\"\\nüèÜ Records per league:\")\n",
                "    print(master_df['_league'].value_counts())\n",
                "else:\n",
                "    print(\"‚ùå No data collected!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Quick Data Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check files\n",
                "print(\"üìÅ Files in raw data directory:\")\n",
                "csv_files = list(RAW_DIR.glob(\"*.csv\"))\n",
                "print(f\"Total files: {len(csv_files)}\")\n",
                "for f in sorted(csv_files):\n",
                "    size_kb = f.stat().st_size / 1024\n",
                "    print(f\"  - {f.name} ({size_kb:.1f} KB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and preview master file\n",
                "master_file = RAW_DIR / \"all_leagues_master.csv\"\n",
                "\n",
                "if master_file.exists():\n",
                "    df = pd.read_csv(master_file)\n",
                "    print(f\"üìä Master dataset: {len(df)} player records\")\n",
                "    print(f\"\\nüìã Columns:\")\n",
                "    print(df.columns.tolist())\n",
                "    print(f\"\\nüîç Sample data:\")\n",
                "    display(df.head())\n",
                "else:\n",
                "    print(\"‚ùå Master file not found. Run scraping cells first!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Find Ghana Players"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Search for Ghana players\n",
                "if 'df' in dir() and not df.empty:\n",
                "    # Find player column\n",
                "    player_col = [c for c in df.columns if 'player' in c.lower()][0]\n",
                "    \n",
                "    print(f\"üá¨üá≠ Searching for Ghana players...\")\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    \n",
                "    found = []\n",
                "    not_found = []\n",
                "    \n",
                "    for player in GHANA_FORWARDS:\n",
                "        last_name = player.split()[-1]\n",
                "        matches = df[df[player_col].astype(str).str.contains(last_name, case=False, na=False)]\n",
                "        \n",
                "        if not matches.empty:\n",
                "            found.append(player)\n",
                "            match_name = matches[player_col].iloc[0]\n",
                "            match_league = matches['_league'].iloc[0] if '_league' in matches.columns else 'Unknown'\n",
                "            print(f\"‚úÖ {player}\")\n",
                "            print(f\"   ‚Üí Found as: {match_name} ({match_league})\")\n",
                "        else:\n",
                "            not_found.append(player)\n",
                "            print(f\"‚ùå {player} - NOT FOUND\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"\\nüìä Found {len(found)}/{len(GHANA_FORWARDS)} Ghana players\")\n",
                "    \n",
                "    if not_found:\n",
                "        print(f\"\\n‚ö†Ô∏è Not found (may be in leagues not scraped):\")\n",
                "        for p in not_found:\n",
                "            print(f\"   - {p}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ‚úÖ Next Steps\n",
                "\n",
                "Data collection is complete! Move on to **Notebook 02** for data processing."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}